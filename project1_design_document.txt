            +--------------------+
            |       CS 311       |
            | PROJECT 1: THREADS |
            |   DESIGN DOCUMENT  |
            +--------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Minh Duc Nguyen (20202026) <ducnm@unist.ac.kr>
Thu Phuong Nguyen (20202027) <phuongnt@unist.ac.kr>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

+ In threads.h, added:
int64_t wakeup_tick;                /* Tick till wake up */
int64_t get_min_sleep_tick(void);   /* The minimum sleep ticks */
void thread_sleep (int64_t);        /* Move the current thread to the sleep queue. */
void thread_wakeup(int64_t);        /* Find a sleeping thread and move it to ready queue */

+ In threads.c, added:
static struct list sleep_list;              /* List of processes in THREAD_SLEEP state (processes are sleeping). */
static int64_t min_sleep_ticks;             /* The minimum sleep ticks */
static void reset_min_sleep_tick(void);     /* Reset the minimum sleep ticks to infinity */
#define min(x, y) (((x) < (y))? (x) : (y))  /* Find min between 2 variables. */
#define minimize(x, y) x = min(x, y)        /* Find the min between 2 variables and then assign to the first variable. */


---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

In timer_sleep(ticks):
    1. First, record the current timer tick count (start).
    2. Assert that interrupts are enabled, ensuring that the system can respond to timer interrupts
    3. If the elapsed time since the start of the timer is less than the specified number of ticks, it proceeds with actual sleep, delegating the task to thread_sleep(start + ticks).

In thread_sleep(ticks):
    1. First, we disable the interrupt
    2. If the current thread is not idle, move it to the sleep list for actual sleeping
    3. Assign the tick to wakeup the thread
    4. Update the global minimum sleep ticks
    5. Mark the thread as BLOCKED and yield the CPU

When timer_interrupt() handler is invoked: 
    1. At each timer tick, it increments the global tick count (ticks).
    2. If the global minimum sleep tick is less than the current tick, some threads need to be waken up
    3. Invokes thread_wakeup(ticks) to move them to the ready list.

In thread_wakeup(ticks):
    1. Disable interrupts
    2. Iterate through the sleep list to find sleeping thread with wakeup_tick <= ticks (the one that should be waken up now since the timer has expired)
    3. Remove them from the sleep list, and unblock them.
    4. Updates the global tick with respect to those still sleeping


>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?
+ Using linked list sleep_list, selectively wakes up threads only when necessary, and efficiently tracks the next earliest wakeup time using get_min_sleep_tick(). 
+ Interrupts are briefly disabled during critical sections to ensure thread safety and prevent race conditions.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?
+ Interrupts are temporarily disabled during critical sections to prevent preemption and ensure that the current thread can complete its operation without interference.
+ Access to shared data such as sleep queue (sleep_list) is synchronized to prevent multiple threads from concurrently modifying them.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?
+ During the execution of critical sections in timer_sleep(), interrupts are disabled to prevent timer_interrupt() from being invoked and potentially interfering with the synchronization process.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?
+ By utilizing linked list (sleep_list) and selectively waking up threads only when necessary, the system avoids busy waiting and optimizes resource utilization.
+ Synchronization such as locks and disabling interrupts during critical sections ensures thread safety and prevents race conditions ==> multiple threads to safely call timer_sleep() simultaneously without risking data corruption or inconsistent behavior.




              SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

+ In threads.h, added:
struct lock *wait_on_lock;                      /* The lock that the thread is waiting on */
struct list donations;                          /* The threads to donate the priority to */
struct list_elem d_elem;                        /* List element on donation list */

void reset_priority(void);                      /* Reset the priority */
void thread_yield_if_not_max(void);             /* Preempt the thread if there is a higher priority thread */
struct thread* thread_highest_priority(void);   /* Get the thread with highest priority */
/* Returns true if the priority of the thread A > B. */
bool thread_cmp_priority(const struct list_elem*, const struct list_elem*, void*);
  

+ In synch.h, added:
/* Returns true if the priority of the thread in
   semaphore A is greater than that of semaphore B. 
*/
bool sync_cmp_priority(const struct list_elem*, const struct list_elem*, void*);


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

>>>>> TODO

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

+ When a thread attempts to acquire a lock, semaphore, or condition variable, 
it is inserted into the appropriate wait queue (donations list for locks or 
waiters list for semaphores and condition variables) based on its priority, 
ensuring that the highest priority thread is positioned at the front of the queue, 
the first to be waked up.

+ If priority donation occurs, where a lower-priority thread holding a lock donates 
its priority to a higher-priority thread waiting on that lock, the priority of the 
waiting thread is dynamically updated to reflect the donated priority. 


>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

+ When a thread attempts to acquire a lock, it checks if the lock is available. 
If the lock is held by another thread, priority donation may occur.

+ The waiting thread inherits the priority of the holder of the lock, allowing 
it to proceed immediately if its new priority is higher than that of any other 
waiting threads.

+ Nested priority donation: where a thread holding a lock is itself waiting on 
another lock, the donation process continues recursively (here, we just need a while loop). 
Each waiting thread inherits the priority of the holder of the lock it is waiting on, 
ensuring that the highest effective priority is maintained throughout the chain of dependencies.

+ Once priority donation is completed, the waiting thread acquires the lock and 
proceeds with its execution. 

+ If the lock is not immediately available, the thread is blocked and added to the 
lock's wait queue until the lock becomes available.


>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

+ Release the lock.

+ The holder's priority need to be updated if it received priority donations. 

+ If the released lock was responsible for donating priority to the holder,
the holder's priority is restored to its original priority level before 
the donation occurred.

+ If a higher-priority thread is waiting on the released lock, it is unblocked 
and moved to the ready queue. 


---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it. Can you use a lock to avoid
>> this race?

+ A potential race condition in thread_set_priority(): multiple threads attempt to 
change the priority of a target thread simultaneously. If thread A sets the priority 
of thread B while thread B is concurrently being scheduled to execute on another 
processor core, inconsistencies in the priority level of thread B may occur, leading 
to unexpected behavior.

+ To avoid this race condition, we promptly disable the interrupt during the priority 
setting process ensuring that only one thread can modify the priority of a target thread 
at a time. 

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

+ In comparison to alternative designs, these features enhance to manage thread priorities, 
prevent priority-related issues,promptly releasing the lock and unblocking the highest 
priority waiting thread ==> efficient resource utilization and prevents priority inversion 
scenarios.

+ By utilizing locks and synchronization mechanisms, we could keep thread safety and 
prevents race conditions in critical operations such as priority donation and modification.




              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

+ In threads.h, added:
#define NICE_DEFAULT 0             /* Default nice value. */
#define RECENT_CPU_DEFAULT 0       /* Default recent_cpu value. */
#define LOAD_AVG_DEFAULT 0         /* Default load_avg value. */
#define FIXED_POINT_1 (1 << 14)    /* The number 1.0 in fix-point format */ 

typedef int fp;   /* A struct to represent real numbers using 17.14 fixed-point format */
int nice;                          /* The nice value of the thread */
fp recent_cpu;                     /* The recent cpu usage in fixed-point format */

/* Fixed-point arithmetics */
fp int_to_fp(int);                 /* Convert int to fp */
int fp_to_int_0(fp);               /* Convert fp to int, rounding toward 0 */
int fp_to_int_nearest(fp);         /* Convert fp to int, rounding to nearest */
fp add_fp(fp, fp);                 /* Add two fix-point numbers */
fp sub_fp(fp, fp);                 /* Subtract two fix-point numbers */
fp add_fp_int(fp, int);            /* Add fix-point number and int */
fp sub_fp_int(fp, int);            /* Subtract fix-point number and int */
fp mul_fp(fp, fp);                 /* Multiply 2 fix-point numbers */
fp div_fp(fp, fp);                 /* Divide 2 fix-point numbers */
fp mul_fp_int(fp, int);            /* Multiply fix-point number and int */
fp div_fp_int(fp, int);            /* Divide fix-point number and int */

/* MLFQS utilities */
int mlfqs_get_priority(fp, int);   /* Get the priority based on recent_cpu and nice value */
fp mlfqs_new_recent_cpu(fp, int);  /* Calculate the new recent_cpu value */
fp mlfqs_new_load_avg(fp);         /* Calculate the new load_avg */
void mlfqs_increase_recent_cpu(void);   /* Increment recent_cpu for the running thread */
void mlfqs_recalculate_all_priority(void);   /* Recalculate priorities for all threads */
void mlfqs_recalculate_all_recent_cpu(void); /* Recalculate recent_cpu for all threads */



---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

Here, we assume the definition of Pintos. (#define TIMER_FREQ 100)

timer     recent_cpu     priority       thread
ticks     A    B    C    A    B    C    to run
-----     --   --   --   --   --   --   ------
 0        0    0    0    63   61   59   A   
 4        4    0    0    62   61   59   A
 8        8    0    0    61   61   59   A
12        12   0    0    60   61   59   B
16        12   4    0    60   60   59   A
20        16   4    0    59   60   59   B
24        16   8    0    59   59   59   A
28        20   8    0    58   59   59   B
32        20   12   0    58   58   59   C
36        20   12   4    58   58   58   A


>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

+ In the scheduler specification ambiguities, there is ambiguity in the precise timing of 
when the recent_cpu and priority values should be updated.

+ To resolve: 
We assume that recent_cpu is incremented every timer tick, and the load average is 
recalculated every second (every TIMER_FREQ ticks).
Priorities are recalculated every 4 timer ticks.

+ It matches the behavior of the MLFQS.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

+ Inside and outside interrupt context: We perform time-critical operations: 
updating the tick count, making immediate scheduling decisions based on priority, 
preempting the current thread if a higher-priority thread is available. 
==> ensure responsiveness, high-priority threads are scheduled with minimal latency.

+ Outside interrupt context: We perform more complex tasks not required to be done at 
each tick and computationally intensive: updating the recent_cpu and load_avg values, 
recalculating priorities for all threads periodically, managing the lists of threads and 
performing context switches. ==> Keep the interrupt handlers short and efficient to reduce 
interrupt latency.


---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

+ Advantages:
     - MLFQS design dynamically adjusts thread priorities based on their recent CPU usage 
     and nice values ==> not starve I/O-bound threads, promoting fairness.
     - Updating recent_cpu and load_avg periodically and not at every tick ==> balances 
     accuracy with performance, avoiding excessive computation overhead.
     -  Fixed-point maths still work well on systems without floating-point hardware.

+ Disadvantages:
     - Recalculate for all threads can be expensive for systems that have a large number of 
     threads.
     - While fixed-point arithmetic is efficient, it lacks the precision and range of 
     floating-point arithmetic, potentially leading to rounding errors and less 
     accurate calculations.

+ Potential improvement:
     - Dynamically adjust the parameters of the scheduling algorithm (weights for recent_cpu 
     and nice values) based on runtime metrics and feedback.
     - Adaptively adjust the recalculation frequency based on system load.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

+ Fixed-point arithmetic is simpler and more efficient than floating-point arithmetic, 
especially on systems without dedicated floating-point hardware, and pintos disabled 
float numbers. 

+ We created an abstraction layer for fixed-point math. The functions and macros for 
fixed-point math can be reused across different parts of the scheduler. It makes code 
more modular, easy to maintain and the main scheduling logic more readable.


               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

It took us a lot of time to understand the requirement of the assignment.

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

Looking at materials from KAIST that professor provided helps us to understand the 
MLFQS and priority scheduling. Now we understand more about how a thread 
works with locks and semaphores.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

If there are more examples/ lectures about multiple donations and MLFQS during class time, 
it would be much helpful. 

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

We don't have any suggestions.

>> Any other comments?

Easier Final Exam please.